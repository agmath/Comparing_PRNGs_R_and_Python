---
title: "Results"
format: html
---

# Introduction

This file allows readers of "Are we rolling the same dice? PRNG Consistency Across R and Python" to reproduce the results included in the paper.

For most functions that we defined, we opted to put it in its own Python or R script for consistency. Also, doing so allows readers to more easily find the details of the given code, if it is desired.

We begin by loading in the required R and Python packages.

```{r}
 
#Install and load the reticulate package which allows R to interface with Python
if (!requireNamespace("reticulate", quietly = TRUE)) {
  install.packages("reticulate")
}
library(reticulate)

#Create a virtual environment to use Python from within R
virtualenv_create("myenv")
use_virtualenv("myenv", required = TRUE)

#Install the tqdm package for tracking completion status of 
#long running code.
py_install("tqdm")  #For timing of code
py_install("scipy") 
py_install("rpy2")
py_install("matplotlib")
py_install("SyncRNG")
```

```{r}
#| warnings: false
#| messages: false

#Install and load the dplyr library which is a data manipulation
#package used often within the tidyverse
if (!requireNamespace("dplyr", quietly = TRUE)) {
  install.packages("dplyr")
}
library(dplyr)

#To show progress bars
if (!requireNamespace("progress", quietly = TRUE)) {
  install.packages("progress")
}
library(progress)

#For formatting strings
library(glue)

if (!requireNamespace("EnvStats", quietly = TRUE)) {
  install.packages("EnvStats")
}
library(EnvStats)
```

Now we load in the needed scripts.

```{r}
#| warnings: false
#| messages: false

#Load in the necessary R script files tjat we created to run our R functions.
source("binned_chi_square_tests_mt.R")
source("calc_descriptive_stats_mt_seqs.R")
source("binned_chi_square_tests_mt.R")
source("serial_corr_calc_mt.R")
source("calc_rank_von_neumann.R")
source("generate_rands_mersenne_twister.R")
```

```{python}

#Import the necessary Python modules to run our functions.
import numpy as np
import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

#For visualizations later in this file
import matplotlib.pyplot as plt

#This module shows progress bars in Python methods
from tqdm import tqdm
tqdm.pandas()

#import our Python scripts
import get_seeds_for_null_distributions
import compare_dists_mt_mt_ks_test as ks_analysis_mt_mt
import compare_dists_lcg_mt_ks_test as ks_analysis_mt_lcg
import compare_dists_pcg_mt_ks_test as ks_analysis_mt_pcg 
import compare_dists_mt_xor_ks_test as ks_analysis_mt_xor 
```

# Default Parameter Values

To be consistent with the paper, throughout this file we generate sequences of $2^{14} - 1$ random numbers in the range of 0 to 99,999 using a seed of 1234. We create some constants to do this consistently.

```{r}
#R Constants
NUM_SEQUENCES_R = 10
SEQ_LENGTH_R = 2^14 - 1 
MIN_RANDOM_VALUE_R = 0
MAX_RANDOM_VALUE_R = 10^5 - 1
SEED_R = 1234
```

```{python}
#Python constants
NUM_SEQUENCES_PYTHON = 10
SEQ_LENGTH_PYTHON = 2**14 - 1 
MIN_RANDOM_VALUE_PYTHON = 0
MAX_RANDOM_VALUE_PYTHON = 10**5 - 1
SEED_PYTHON = 1234
XOR_SHIFT_SEED2 = 5678  #The xor algorithm requires two seeds
BAR_HATCH = '\\\\\\\\'  #For displaying the results in a histogram
```

# Descriptive Statistics of Random Sequences in R

We generate ten sequences using R, as well as the mean, standard deviation, coefficient of variation, min, and max of each sequence.

The function below spawns a chain which eventually calls the R `sample` function with replacement. As mentioned in the paper, R's `sample` method uses the Mersenne Twister Algorithm (MTA) by default.

-   `sample(0:max_rand, size = seq_length, replace = TRUE)`

```{r}
calc_descriptives_stats_mt_seqs(num_seqs = NUM_SEQUENCES_R, 
                                seq_length = SEQ_LENGTH_R, 
                                min_rand = MIN_RANDOM_VALUE_R,
                                max_rand = MAX_RANDOM_VALUE_R,
                                seed = SEED_R)
```

Here we calculate the theoretical mean, standard deviation, and coefficient of variation for uniform random variables. We do this to provide a comparison factor for those values shown in the previous table.

```{python}
a = MIN_RANDOM_VALUE_PYTHON
b = MAX_RANDOM_VALUE_PYTHON

mean_theoretical = (a+b)/2
std_theoretical = (b-a)/12**0.5
cv_theoretical = std_theoretical/mean_theoretical

print(f"Theoretical Values for a Uniform({a}, {b}) Random Variable:")
print(f"Mean: {mean_theoretical}")
print(f"Standard Deviation: {std_theoretical}")
print(f"Coefficient of Variation: {cv_theoretical}")

```

# Chi-Square Goodness of Fit

We wish to assess if the data randomly generated by R's `sample` method that by default runs the MT algorithm is uniformly distributed. So first we perform a Chi-square goodness-of-fit test.

The number of bins are determined by Sturge's rule. For 9 of the 10 sequences, we do not reject the null hypothesis below. Thus, it is plausible that the R sequences generated by the sample method are uniformly distributed.

-   $H_0:$ The sequence is uniformly distributed.
-   $H_A:$ The sequence is not uniformly distributed.

```{r}
#Sturge's Rule gives 15 bins.
num_bins_sturges = ceiling(1 + log2(SEQ_LENGTH_R))

binned_chi_square_tests_mt(num_seqs = NUM_SEQUENCES_R, 
                          seq_length = SEQ_LENGTH_R, 
                          max_rand = MAX_RANDOM_VALUE_R,
                          seed = SEED_R, 
                          num_bins = num_bins_sturges)
```

# Serial Correlations

Our goal in this section is to see if the numbers within a sequence generated by the sample method are exhibiting underlying patterns.

## Autocorrelation: Rank von Neumann test

Since the Von Neumann test assumes normality of the sequence, we instead perform a Rank Von Neumann test on each of the 10 sequences. In each case, the test results did not reject the null hypothesis of having no autocorrelation in the sequence. The majority of our p-values were quite large (the smallest two were 6% and 21%, and the next smallest was over 50%).

It should be noted, however, that approximately 15% of our observations were repeated values. This percentage is what would be expected with our sample size and range. However, the Rank von Neumann test assumes that there are no ties in the data. So in the next section, we explore further by using Pearson's Correlation Coefficient to assess the autocorrelation.

```{r, warning=TRUE}
rank_von_neumann_df <- calc_rank_von_neumann(num_seqs = NUM_SEQUENCES_R, 
                                      seq_length = SEQ_LENGTH_R, 
                                      min_rand = MIN_RANDOM_VALUE_R,
                                      max_rand = MAX_RANDOM_VALUE_R,
                                      seed = SEED_R)

# We order the results so that the largest and smallest autocorrelations are evident.
#serial_df[order(serial_df$serial_correlation), ]
rank_von_neumann_df[order(rank_von_neumann_df$p_value), ]
```

## Pearson's Correlation Coefficient

We generate serial correlations for 10 random sequences. So we find the correlation coefficient between each sequence and itself but with a lag/shift of 1. The serial correlations for all sequences are very close to zero: within the range of -0.0146 to 0.00481, with some small positive and negative values.

As in the previous section, notice that the smallest two p-values are approximately 6% and 21%, and the next smallest is over 50%. (The p-values are very similar as in the previous section but not exactly the same.) So for all 10 of the sequences, we do not reject the null hypothesis below. Thus, it is plausible that each sequence and its corresponding lagged sequence are not correlated.

-   $H_0:$ The lagged sequence and original sequence have no correlation.
-   $H_A:$ The lagged sequence and original sequence are correlated.

```{r}
serial_df <- serial_corr_calc_mt(num_seqs = NUM_SEQUENCES_R, 
                                      seq_length = SEQ_LENGTH_R, 
                                      min_rand = MIN_RANDOM_VALUE_R,
                                      max_rand = MAX_RANDOM_VALUE_R,
                                      seed = SEED_R, 
                                      num_lag = 1)

# We order the results so that the largest and smallest autocorrelations are evident.
serial_df[order(serial_df$serial_correlation), ]
```

Now we consider the correlations between a randomly generated sequence and a lag of 1 up until 50% of the sequence length. For each, we run a test to determine which hypothesis is possible.

-   $H_0:$ The lagged sequence and original sequence have no correlation.
-   $H_A:$ The lagged sequence and original sequence are correlated.

```{r}
# for loop for lag

#Create a column recording the lag used to generate each correlation.
serial_df$lag <- 1

#We'll use a lag of 1 to 50% of the sequence length.
max_lag = as.integer(SEQ_LENGTH_R/2)

# Initialize progress bar
pb <- progress_bar$new(total = max_lag, 
                       format = "[:bar] :percent :elapsed ETA: :eta") 

for (lag in 2:max_lag){
  
  serial_df2 <- serial_corr_calc_mt(num_seqs = NUM_SEQUENCES_R, 
                                        seq_length = SEQ_LENGTH_R, 
                                        min_rand = MIN_RANDOM_VALUE_R,
                                        max_rand = MAX_RANDOM_VALUE_R,
                                        seed = SEED_R, 
                                        num_lag = lag)
  serial_df2$lag = lag
  
  serial_df <- rbind(serial_df, serial_df2)
  
  # Update the progress bar
  pb$tick()
}
```

Below are the results of the lag analysis, in descending order of p-value.

```{r}
serial_df[order(serial_df$p_value, decreasing = TRUE),]
```

We see that 4.97% of the tests lead to a rejection of the null hypothesis, which is to be expected if the lagged sequences and original sequences have no correlation.

```{r}
#Count up the number of lag/sequence pairs that would lead to a rejection of the
#null hypothesis, ie, for which it is plausible that no correlation exists.
#So reject_H0 would be TRUE for these pairs.
num_could_be_correlated = sum(serial_df$reject_H0)
num_tests = nrow(serial_df)
perc_rejected = num_could_be_correlated/nrow(serial_df)*100

cat("Of the ", num_tests, " tests, it is plausible that ", 
    num_could_be_correlated, 
    " sequences and its lagged variant could be correlated (", 
    perc_rejected, "%).\n", sep="")

summary(serial_df$serial_correlation)
```

Here are histograms showing the lagged results. They all support that there does not appear to be autocorrelation amongst the numbers in each sequence for the lags tested.

For the first histogram below, we see that the correlation coefficients appear to be centered at 0 and follow a bell shaped and symmetric distribution.

```{r}

hist(serial_df$serial_correlation,
     main = "Serial Correlations Of Many Lags", # Main title
     xlab = "Serial Correlation",                            # X-axis label
     ylab = "Frequency",                        # Y-axis label
     col = "lightblue",                         # Bar color
     border = "black")                          # Border color of bars

```

In the next histogram, we see that the p-values appear to have a uniform distribution as well.

```{r}

#Divide the results into those where we reject H0 and those where we do not.
serial_reject_H0 = serial_df[serial_df$reject_H0, ]
serial_do_not_reject_H0 = serial_df[!serial_df$reject_H0, ]

percent_not_rejected = round(nrow(serial_do_not_reject_H0)/nrow(serial_df)*100, digits = 2)

title = formatted_string_glue <- glue("p-Values of Serial Correlation Tests\nH0 not rejected in {percent_not_rejected}% of the results.")

hist(serial_do_not_reject_H0$p_value,
     main = title, # Main title
     xlab = "P-Value",      # X-axis label
     ylab = "Frequency",    # Y-axis label
     col = "lightblue",     # Bar color
     border = "black",      # Border color of bars
     xlim = c(0, 1), 
     breaks = seq(0, 1, by = 0.05))

hist(serial_reject_H0$p_value,
     xlab = "P-Value",         # X-axis label
     ylab = "Frequency",      # Y-axis label
     col = "red",             # Bar color
    border = "black",         # Border color of bars 
     density = 20,            # Add slanted lines
     angle = 135,             # Angle of slashes
     breaks = seq(0, 1, by = 0.05),
     add = TRUE)              # X-axis limits from 0 to 1


legend("center", 
       legend = c("H0 rejected (a correlation is plausible)", 
                  "H0 not rejected (no correlation is plausible)"), 
       fill = c("red", "lightblue"),
       border  = "black",
       density = c(20, NA),     # slashes only on the rejected (red) bars
       angle   = c(135, NA))

```

Finally, below are the lags of the sequences that led to a rejection as well as the lags of those that did not. The lags for both appear to be uniformly distributed.

```{r}

title = formatted_string_glue <- glue("Lags in Serial Correlation Tests\nH0 not rejected in {percent_not_rejected}% of the results.")


hist(serial_do_not_reject_H0$lag,
     main = "Lags of Serial Correlation Tests", # Main title
     xlab = "Lag",                            # X-axis label
     ylab = "Frequency",                        # Y-axis label
     col = "lightblue",                         # Bar color
     border = "black")                          # Border color of bars

hist(serial_reject_H0$lag,
     xlab = "Lag",                            # X-axis label
     ylab = "Frequency",                        # Y-axis label
     col = "red",                         # Bar color
     border = "black",                           # Border color of bars
     density = 20,          # Add slanted lines
     angle = 135,            # Angle of slashes
     add = TRUE)

legend("center", 
       legend = c("H0 rejected (a correlation is plausible)", 
                  "H0 not rejected (no correlation is plausible)"), 
       fill = c("red", "lightblue"),
       density = c(20, NA),     # slashes only on the rejected (red) bars
       angle   = c(135, NA))


```

# Comparing Mersenne Twister in R and Mersenne Twister in Python

We begin by comparing two sequences of random numbers in R and Python of the same length (2\^14 - 1 = 16,343 values) using the same seed (1234) in the range 0 to 99,999. In R, we used the R function sample() with replacement and in Python we used random.randint(). We then perform a two-sample Kolmogorov–Smirnov (KS) test. The KS statistic measures the maximum difference between the cumulative distribution functions of the two sequences. A small KS statistic and a high p-value indicate that the two sequences do not differ significantly in distribution. The KS statistic was 0.0065 and the p-value was 0.8735. The high p-value indicates that the distributions of the R and Python sequences are statistically indistinguishable

```{python}
#Do one test - This returns the KS Test Statistic, the p-value, and a boolean:
#    True:   The null was rejected at a level of 0.05
#    False:  The null was not rejected at a level of 0.05
ks_analysis_mt_mt.compare_dists_mt_mt_ks_test(seq_length = SEQ_LENGTH_PYTHON, 
                            min_rand = MIN_RANDOM_VALUE_PYTHON,
                            max_rand = MAX_RANDOM_VALUE_PYTHON,
                            seed = SEED_PYTHON)
```

We repeat the previous experiment with 10,000 random seeds. For each of these trials, we performed a KS test. As seen in the histogram created below, we rejected the null hypothesis approximately 5% of the time (5.15%). This is exactly what would be expected when the null hypothesis is true.

```{python}
#Run many tests to build a null distribution
ks_results_df_mt_mt = ks_analysis_mt_mt.generate_mt_mt_ks_test_null_distribution(seq_length = SEQ_LENGTH_PYTHON, 
                                                                   min_rand = MIN_RANDOM_VALUE_PYTHON,
                                                                   max_rand = MAX_RANDOM_VALUE_PYTHON)

#print the percentage of trials where the null hypothesis is rejected
perc_rejections =  np.count_nonzero(ks_results_df_mt_mt['reject_H0'])/len(ks_results_df_mt_mt)*100
print(f"H0 is rejected in {perc_rejections:.4f}% of the trials")

ks_results_df_mt_mt.head(10)
```

Below we show a histogram of the p-values and KS Test statistics.

```{python}
#Histogram of the results

#set up subplots and overall title
fig, ax = plt.subplots(2,1, facecolor='white')
fig.subplots_adjust(hspace=0.5)
fig.suptitle(f'KS Test (MT in R vs. MT in Python) - {len(ks_results_df_mt_mt):,} Runs')


#Divide the results into where the p-value is high or low.
#We'll draw the results where H0 is rejected in red.
#We'll draw the results where we fail to reject H0 in blue.
ks_results_df_mt_mt_red = ks_results_df_mt_mt[ks_results_df_mt_mt["p_value"] < 0.05]
ks_results_df_mt_mt_blue = ks_results_df_mt_mt[ks_results_df_mt_mt["p_value"] >= 0.05]

#Draw the histograms of the test statistic where H0 is rejected
res0 = ax[0].hist(ks_results_df_mt_mt_red["ks_test_statistic"], 
                  bins=np.arange(0,0.0325,.0025),
                  edgecolor='black', 
                  color='red', 
                  hatch = BAR_HATCH,
                  label="Reject H0 (The R and Python MT Sequences have differing distributions)");

#Draw the histograms of the test statistic where we fail to reject H0                  
res0 = ax[0].hist(ks_results_df_mt_mt_blue["ks_test_statistic"], 
                  bins=np.arange(0,0.0325,.0025),
                  edgecolor='black', 
                  color='blue',
                  label="Do not reject H0 (The R and Python MT Sequences have the same distribution)");                  
ax[0].set_ylabel("Frequency")
ax[0].set_xlabel("KS Test Statistics") 
ax[0].set_ylim(0,4000)
 
# Legend above the first subplot
ax[0].legend(
    loc='lower center',       
    bbox_to_anchor=(0.5, 1.15), 
    frameon=False    
)


#Draw the histogram of p-values that lead to a rejection of H0.
#Then those that do not.
res1 = ax[1].hist(ks_results_df_mt_mt_red["p_value"],
                  bins=np.arange(0,1.05,.05),
                  edgecolor='black', 
                  color='red',
                  hatch = BAR_HATCH);                  
res1 = ax[1].hist(ks_results_df_mt_mt_blue["p_value"],
                  bins=np.arange(0,1.05,.05),
                  edgecolor='black', 
                  color='blue');

ax[1].set_ylim(0,600)
ax[1].set_xlabel("P-Values")
ax[1].set_ylabel("Frequency")

 
# Leave space at the top for the title + legend
plt.tight_layout(rect=[0, 0, 1, 1])

plt.savefig(
    "data/KSTest_NullDistributions_MT_MT.png",
    bbox_inches="tight"
)
plt.show()

```

# Comparing Mersenne Twister in Python and the Permutation Congruence Generator Algorithm

We compared a sequence of 2\^14 - 1 random integers generated in Python using the Mersenne-Twister algorithm with a sequence generated in Python using PCG. We used PCG64DXSM, which will become the default PRNG in future NumPy releases. Thus, this code implements the PCG64DXSM BitGenerator to produce the random PCG sequences.

After performing a KS test, we received a small KS statistic (0.0091) and a high p-value (0.5041) which indicate that the MTA-generated sequence and the PCG-generated sequence do not differ significantly in distribution. This high p-value indicates that the distributions of the R and Python sequences are statistically indistinguishable

```{python}

#Do one test - This returns the KS Test Statistic, the p-value, and a boolean:
#    True:   The null was rejected at a level of 0.05
#    False:  The null was not rejected at a level of 0.05
ks_analysis_mt_pcg.compare_dists_pcg_mt_ks_test(seq_length = SEQ_LENGTH_PYTHON, 
                                                min_rand = MIN_RANDOM_VALUE_PYTHON,
                                                max_rand = MAX_RANDOM_VALUE_PYTHON,
                                                rand_seed = SEED_PYTHON)
```

We repeat the previous experiment with 10,000 random seeds. For each of these trials, we performed a KS test. As seen in the histogram created below, we rejected the null hypothesis approximately 5% of the time (4.82%). This is exactly what would be expected when the null hypothesis is true.

```{python}
#Run many tests to build a null distribution
ks_results_df_mt_pcg = ks_analysis_mt_pcg.generate_pcg_mt_ks_test_null_distribution(seq_length = SEQ_LENGTH_PYTHON, 
                                                                          min_rand = MIN_RANDOM_VALUE_PYTHON,
                                                                          max_rand = MAX_RANDOM_VALUE_PYTHON)
                                                                          
#print the percentage of trials where the null hypothesis is rejected
perc_rejections_mt_pcg =  np.count_nonzero(ks_results_df_mt_pcg['reject_H0'])/len(ks_results_df_mt_pcg) * 100
print(f"H0 is rejected in {perc_rejections_mt_pcg:.4f}% of the trials")                                                                          
ks_results_df_mt_pcg.head(10)
```

Below we show a histogram of the p-values and KS Test statistics.

```{python}

#Divide the results into where the p-value is high or low.
#We'll draw the results where H0 is rejected in red.
#We'll draw the results where we fail to reject H0 in blue.
ks_results_df_mt_pcg_red = ks_results_df_mt_pcg[ks_results_df_mt_pcg["p_value"] < 0.05]
ks_results_df_mt_pcg_blue = ks_results_df_mt_pcg[ks_results_df_mt_pcg["p_value"] >= 0.05]

#set up subplots and overall title
fig, ax = plt.subplots(2,1, facecolor='white')
fig.subplots_adjust(hspace=0.5)
fig.suptitle(f'KS Test (MT in Python vs. PCG in Python) - {len(ks_results_df_mt_pcg):,} Runs')

#Draw the histogram of the test statistic where H0 is rejected
res0 = ax[0].hist(ks_results_df_mt_pcg_red["ks_test_statistic"], 
                  edgecolor='black', 
                  color='red',
                  hatch = BAR_HATCH,          
                  bins=np.arange(0,0.0325,.0025),
                  label="Reject H0 (The PCG and MT Sequences have differing distributions)");
                  
#Draw the histogram of the test statistic where we fail to reject H0                    
res0 = ax[0].hist(ks_results_df_mt_pcg_blue["ks_test_statistic"], 
                  edgecolor='black', 
                  color='blue',
                  bins=np.arange(0,0.0325,.0025),
                  label="Do not Reject H0 (The PCG and MT Sequences have the same distributions)");
ax[0].set_xlabel("Test Statistics")
ax[0].set_ylabel("Frequency")

# Legend above the first subplot
ax[0].legend(
    loc='lower center',       
    bbox_to_anchor=(0.5, 1.15),  
    #ncol=2,                      
    frameon=False                 
)
#Draw the histogram of p-values that lead to a rejection of H0.
#Then those that do not.
res1 = ax[1].hist(ks_results_df_mt_pcg_red["p_value"],
                  bins=np.arange(0,1.05,.05),
                  edgecolor='black', 
                  color='red',
                  hatch = BAR_HATCH);    
res1 = ax[1].hist(ks_results_df_mt_pcg_blue["p_value"],
                  bins=np.arange(0,1.05,.05),
                  edgecolor='black', 
                  color='blue');    
ax[1].set_xlabel("P-Values")
ax[1].set_ylabel("Frequency")

# Leave space at the top for the title + legend
plt.tight_layout(rect=[0, 0, 1, 1])

plt.savefig("data/KSTest_NullDistributions_MT_PCG.png")
plt.show();
```

# Comparing Mersenne Twister in Python and the XorShift128+ Algorithm

We compare two random number sequences: both generated in Python, but one relying on MTA and the other on the XorShift128+ algorithm. Both the R and Python implementations of the XorShift128+ algorithm need two 64-bit seeds. We used the seeds 1234 and 5678, respectively. Both sequences consisted of 2\^14 - 1 random integers scaled to the range \[0, 99,999\].

To assess the similarity of their distributions, we performed a two-sample KS test. The KS statistic was 0.0075, and the p-value was 0.7420. These results indicate no statistically significant difference between the two distributions at the 5% significance level.

After performing a KS test, we received a small KS statistic (0.0075) and a high p-value (0.7420) which indicate that the MTA-generated sequence and the PCG-generated sequence do not differ significantly in distribution. This high p-value indicates that the distributions of the R and Python sequences are statistically indistinguishable.

```{python}

#Do one test - This returns the KS Test Statistic, the p-value, and a boolean:
#    True:   The null was rejected at a level of 0.05
#    False:  The null was not rejected at a level of 0.05
ks_analysis_mt_xor.compare_mt_xor_ks_test(seq_length = SEQ_LENGTH_PYTHON, 
                       min_rand = MIN_RANDOM_VALUE_PYTHON,
                       max_rand = MAX_RANDOM_VALUE_PYTHON,
                       rand_seed_mt = SEED_PYTHON, 
                       rand_seed_xor1 = SEED_PYTHON, 
                       rand_seed_xor2 = XOR_SHIFT_SEED2)
                          
```

We now repeat the previous experiment with 5,000 pairs of random seeds. For each of these trials, we performed a KS test. As seen in the histogram created below, we rejected the null hypothesis approximately 5% of the time (4.760%). This is exactly what would be expected when the null hypothesis is true.

```{python}
#Run many tests to build a null distribution
ks_results_df_mt_xor = ks_analysis_mt_xor.generate_xor_mt_ks_test_null_distribution(seq_length = SEQ_LENGTH_PYTHON, 
                                                             min_rand = MIN_RANDOM_VALUE_PYTHON,
                                                             max_rand = MAX_RANDOM_VALUE_PYTHON)
                                                             
#print the percentage of trials where the null hypothesis is rejected
perc_rejections_mt_xor =  np.count_nonzero(ks_results_df_mt_xor['reject_H0'])/len(ks_results_df_mt_xor) * 100
print(f"H0 is rejected in {perc_rejections_mt_xor:.4f}% of the trials")              

ks_results_df_mt_xor.head(10)
```

Below we show a histogram of the p-values and KS Test statistics.

```{python}

#Divide the results into where the p-value is high or low.
#We'll draw the results where H0 is rejected in red.
#We'll draw the results where we fail to reject H0 in blue.
ks_results_df_mt_xor_red =ks_results_df_mt_xor[ks_results_df_mt_xor["p_value"] < 0.05]
ks_results_df_mt_xor_blue = ks_results_df_mt_xor[ks_results_df_mt_xor["p_value"] >= 0.05]

#set up subplots and overall title
fig, ax = plt.subplots(2,1, facecolor='white')
fig.subplots_adjust(hspace=0.5)
fig.suptitle(f'KS Test (MT in Python vs. XorShift128+ in Python) - {len(ks_results_df_mt_xor):,} Runs')

#Draw the histogram of the test statistic where H0 is rejected
res0 = ax[0].hist(ks_results_df_mt_xor_red["ks_test_statistic"], 
                  edgecolor='black', 
                  color='red',
                  linewidth=0.25,
                  bins=np.arange(0,0.0325,.0025),
                  hatch=BAR_HATCH,
                  label="Reject H0 (The MT and XORShift128+ Sequences have differing distributions)");
                  
#Draw the histogram of the test statistic where we fail to reject H0                    
res0 = ax[0].hist(ks_results_df_mt_xor_blue["ks_test_statistic"], 
                  edgecolor='black', 
                  color='blue',
                  bins=np.arange(0,0.0325,.0025),
                  label="Do not Reject H0 (The MT and XORShift128+ Sequences have the same distributions)");
                  
ax[0].set_xlabel("Test Statistics")
ax[0].set_ylabel("Frequency")

# Legend above the first subplot
ax[0].legend(
    loc='lower center',       
    bbox_to_anchor=(0.5, 1.15),  
    #ncol=2,                      
    frameon=False                 
)
#Draw the histogram of p-values that lead to a rejection of H0.
#Then those that do not.
res1 = ax[1].hist(ks_results_df_mt_xor_red["p_value"],
                  bins=np.arange(0,1.05,.05),
                  edgecolor='black', 
                  color='red',
                  hatch=BAR_HATCH);    
res1 = ax[1].hist(ks_results_df_mt_xor_blue["p_value"],
                  bins=np.arange(0,1.05,.05),
                  edgecolor='black', 
                  color='blue');    
ax[1].set_xlabel("P-Values")
ax[1].set_ylabel("Frequency")

# Leave space at the top for the title + legend
plt.tight_layout(rect=[0, 0, 1, 1])

plt.savefig("data/KSTest_NullDistributions_MT_XOR.png")
plt.show();
```

# Comparing Mersenne Twister in Python and the Linear Congruence Generator Algorithm

Finally, although not included in our paper, we also compare the MT algorithm in Python and the deterministic LCG Algorithm coded in Python. We compare two random number sequences: both generated in Python.

To assess the similarity of their distributions, we performed a two-sample KS test. The KS statistic was 0.0075, and the p-value was 0.7420. These results indicate no statistically significant difference between the two distributions at the 5% significance level.

After performing a KS test, we received a small KS statistic (0.0111) and a high p-value (0.2622) which indicate that the MTA-generated sequence and the LCG-generated sequence do not differ significantly in distribution. This high p-value indicates that the distributions of the sequences are statistically indistinguishable.

```{python}

#Do one test - This returns the KS Test Statistic, the p-value, and a boolean:
#    True:   The null was rejected at a level of 0.05
#    False:  The null was not rejected at a level of 0.05

#NOTE: We use standard parameters (multiplier = 1664525, increment = 1013904223, modulus = 2³²)
ks_analysis_mt_lcg.compare_dists_lcg_mt_ks_test(seq_length = SEQ_LENGTH_PYTHON, 
                                                    min_rand = MIN_RANDOM_VALUE_PYTHON,
                                                    max_rand = MAX_RANDOM_VALUE_PYTHON,
                                                    rand_seed = SEED_PYTHON,
                                                    a = 1664525, 
                                                    c = 1013904223, 
                                                    m = 2**32)
                           
```

As before, we now repeat the previous experiment with 10,000 random seeds. For each of these trials, we performed a KS test. As seen in the histogram created below, we rejected the null hypothesis approximately 5% of the time (5.04%). This is exactly what would be expected when the null hypothesis is true.

```{python}
#Run many tests to build a null distribution
ks_results_df_mt_lcg = ks_analysis_mt_lcg.generate_lcg_mt_ks_test_null_distribution(seq_length = SEQ_LENGTH_PYTHON, 
                                                                          min_rand = MIN_RANDOM_VALUE_PYTHON,
                                                                          max_rand = MAX_RANDOM_VALUE_PYTHON,
                                                                          a = 1664525, 
                                                                          c = 1013904223, 
                                                                          m = 2**32)
                                                                          
#print the percentage of trials where the null hypothesis is rejected
perc_rejections_mt_lcg =  np.count_nonzero(ks_results_df_mt_lcg['reject_H0'])/len(ks_results_df_mt_lcg) * 100
print(f"H0 is rejected in {perc_rejections_mt_lcg:.4f}% of the trials")          

ks_results_df_mt_lcg.head(10)
```

Below we show a histogram of the p-values and KS Test statistics.

```{python}
#set up subplots and overall title
fig, ax = plt.subplots(2,1, facecolor='white')
fig.subplots_adjust(hspace=0.5)
fig.suptitle(f'KS Test (MT in Python vs. LCG in Python) - {len(ks_results_df_mt_lcg):,} Runs')


#Divide the results into where the p-value is high or low.
#We'll draw the results where H0 is rejected in red.
#We'll draw the results where we fail to reject H0 in blue.
ks_results_df_mt_lcg_red = ks_results_df_mt_lcg[ks_results_df_mt_lcg["p_value"] < 0.05]
ks_results_df_mt_lcg_blue = ks_results_df_mt_lcg[ks_results_df_mt_lcg["p_value"] >= 0.05]

#Draw the histograms of the test statistic where H0 is rejected
res0 = ax[0].hist(ks_results_df_mt_lcg_red["ks_test_statistic"], 
                  bins=np.arange(0,0.0325,.0025),
                  edgecolor='black', 
                  color='red', 
                  hatch = BAR_HATCH,                  
                  label="Reject H0 (The Python MT and LCG Sequences have differing distributions)");

#Draw the histograms of the test statistic where we fail to reject H0                  
res0 = ax[0].hist(ks_results_df_mt_lcg_blue["ks_test_statistic"], 
                  bins=np.arange(0,0.0325,.0025),
                  edgecolor='black', 
                  color='blue',
                  label="Do not reject H0 (he Python MT and LCG Sequences have differing distributions)");                  
ax[0].set_ylabel("Frequency")
ax[0].set_xlabel("KS Test Statistics") 
ax[0].set_ylim(0,4000)

 # Legend above the first subplot
ax[0].legend(
    loc='lower center',       
    bbox_to_anchor=(0.5, 1.15), 
    frameon=False    
)

#Draw the histogram of p-values that lead to a rejection of H0.
#Then those that do not.
res1 = ax[1].hist(ks_results_df_mt_lcg_red["p_value"],
                  bins=np.arange(0,1.05,.05),
                  edgecolor='black', 
                  color='red',
                  hatch = BAR_HATCH);                  
res1 = ax[1].hist(ks_results_df_mt_lcg_blue["p_value"],
                  bins=np.arange(0,1.05,.05),
                  edgecolor='black', 
                  color='blue');


ax[1].set_ylim(0,600)
ax[1].set_xlabel("P-Values")
ax[1].set_ylabel("Frequency")

# Leave space at the top for the title + legend
plt.tight_layout(rect=[0, 0, 1, 1])

plt.savefig("data/KSTest_NullDistributions_Python_MT_Python_LCG.png")

plt.show();
res0
```


# Identical Sequences in R and Python using the SyncRNG 

Here is an example of how to use Identical Sequences uses the SyncRNG package.

```{r}
#Install the pcg package if needed
if (!requireNamespace("SyncRNG", quietly = TRUE)) {
  install.packages("SyncRNG")
}
library(SyncRNG)
options(digits = 20)

s <- SyncRNG(seed=SEED_R)


for (i in 1:10)
  cat(s$randi(), '\n')
```

```{python}
from SyncRNG import SyncRNG


s = SyncRNG(seed=SEED_PYTHON)
for _ in range(10):
  print(s.randi())
```








